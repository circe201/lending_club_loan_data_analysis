{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'./input/lending-club-loan-data/loan_encoded.csv', low_memory=False)\n",
    "data = data.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1961647\n",
       "1     299021\n",
       "Name: Loan_Category_Bad, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Loan_Category_Bad'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1 = data.fillna(0)\n",
    "#data1 = np.nan_to_num(data1)\n",
    "#data = pd.DataFrame(data1, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_good_loans = data.loc[data['Loan_Category_Bad']==0]\n",
    "data_bad_loans = data.loc[data['Loan_Category_Bad']==1]\n",
    "\n",
    "data_good_loans = data_good_loans.iloc[0:290000]\n",
    "data_final = pd.concat([data_good_loans, data_bad_loans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = 0\n",
    "#end = 50000\n",
    "\n",
    "# split files into 10 parts with each file having around 200k records\n",
    "#for i in range(2):\n",
    "#    data[start:end].to_csv(str('loan_encoded')+str(i)+str('.csv'))\n",
    "#    start = end + 1\n",
    "#    end = end + 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_final = data_final.drop(['tot_coll_amt'],axis=1)\n",
    "#data_final = data_final.drop(['all_util','bc_util','mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl','mort_acc','mths_since_recent_bc','mths_since_recent_bc_dlq','mths_since_recent_inq','mths_since_recent_revol_delinq','num_accts_ever_120_pd','num_actv_bc_tl','num_actv_rev_tl','num_bc_sats','num_bc_tl','num_il_tl','num_op_rev_tl'],axis=1)\n",
    "#data_final = data_final.drop(['pct_tl_nvr_dlq','percent_bc_gt_75'],axis=1)\n",
    "#data_final = data_final.drop(['pct_tl_nvr_dlq','percent_bc_gt_75'],axis=1)\n",
    "#data_final = np.where(data_final.values >= np.finfo(np.float64).max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_final['Loan_Category_Bad']\n",
    "X = data_final.drop(['Loan_Category_Bad'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in test & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainY, testY = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MachineLearning\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced', solver='lbfgs')\n",
    "lr.fit(trainX, trainY)\n",
    "predictedY = lr.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72320,   541],\n",
       "       [  867, 73528]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(testY, predictedY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9904384201662411\n",
      "0.9883459909940184\n",
      "0.9905162194201962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "print (accuracy_score(testY, predictedY))\n",
    "print (recall_score(testY, predictedY))\n",
    "print (f1_score(testY, predictedY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save model to file\n",
    "pickle.dump(lr, open(\"lendingclub.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert X,y dataframes to np.array\n",
    "\n",
    "X_list = X.values\n",
    "y_list = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X_list,y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0923 14:35:50.199903 10448 deprecation_wrapper.py:119] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0923 14:35:50.323041 10448 deprecation_wrapper.py:119] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0923 14:35:50.340024 10448 deprecation_wrapper.py:119] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0923 14:35:50.343024 10448 deprecation_wrapper.py:119] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0923 14:35:50.371023 10448 deprecation_wrapper.py:119] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0923 14:35:50.377024 10448 deprecation.py:323] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0923 14:35:51.653234 10448 deprecation_wrapper.py:119] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 309235 samples, validate on 132530 samples\n",
      "Epoch 1/5\n",
      "309235/309235 [==============================] - 10s 33us/step - loss: 7.8463 - acc: 0.5078 - val_loss: 7.8480 - val_acc: 0.5077\n",
      "Epoch 2/5\n",
      "309235/309235 [==============================] - 7s 23us/step - loss: 7.8463 - acc: 0.5078 - val_loss: 7.8480 - val_acc: 0.5077\n",
      "Epoch 3/5\n",
      "309235/309235 [==============================] - 7s 22us/step - loss: 7.8463 - acc: 0.5078 - val_loss: 7.8480 - val_acc: 0.5077\n",
      "Epoch 4/5\n",
      "309235/309235 [==============================] - 7s 23us/step - loss: 7.8463 - acc: 0.5078 - val_loss: 7.8480 - val_acc: 0.5077\n",
      "Epoch 5/5\n",
      "309235/309235 [==============================] - 7s 22us/step - loss: 7.8463 - acc: 0.5078 - val_loss: 7.8480 - val_acc: 0.5077\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 400)               52400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 52,801\n",
      "Trainable params: 52,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "#model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit (trainX,trainY, epochs=5, batch_size=64, validation_split=0.3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 72565]\n",
      " [    0 74691]]\n",
      "0.5072187211386973\n",
      "1.0\n",
      "0.6730525756148992\n"
     ]
    }
   ],
   "source": [
    "predictedY = model.predict(testX)\n",
    "\n",
    "print (confusion_matrix(testY, predictedY))\n",
    "print (accuracy_score(testY, predictedY))\n",
    "print (recall_score(testY, predictedY))\n",
    "print (f1_score(testY, predictedY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
